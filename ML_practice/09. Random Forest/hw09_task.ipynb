{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxs2-KrwXHIJ"
   },
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[HSE][ML][MS][HW09] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost.\n",
    "\n",
    "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "igibYsAlXHIN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL0piKVuXHIP"
   },
   "source": [
    "### Задание 1 (2 балла)\n",
    "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bagging. Элементы, которые не вошли в новую обучающую выборку, образуют out-of-bag выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
    "\n",
    "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
    "\n",
    "#### Методы\n",
    "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bagging. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
    "\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u3nZDhW3XHIO"
   },
   "outputs": [],
   "source": [
    "def gini(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Считает коэффициент Джини для массива меток x.\n",
    "    \"\"\"\n",
    "    labels = np.unique(x)\n",
    "    res = 0\n",
    "    for y in labels:\n",
    "        p = np.sum(x == y) / x.shape[0]\n",
    "        res += p * (1 - p)\n",
    "    return res\n",
    "\n",
    "def entropy(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Считает энтропию для массива меток x.\n",
    "    \"\"\"\n",
    "    labels = np.unique(x)\n",
    "    res = 0\n",
    "    for y in labels:\n",
    "        p = np.sum(x == y) / x.shape[0]\n",
    "        res -= p * np.log(p)\n",
    "    return res\n",
    "\n",
    "def gain(all_y: np.ndarray, left_y: np.ndarray, right_y: np.ndarray, criterion: str) -> float:\n",
    "    \"\"\"\n",
    "    Считает информативность разбиения массива меток.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    left_y : np.ndarray\n",
    "        Левая часть разбиения.\n",
    "    right_y : np.ndarray\n",
    "        Правая часть разбиения.\n",
    "    criterion : Callable\n",
    "        Критерий разбиения.\n",
    "    \"\"\"\n",
    "    if criterion == 'gini':\n",
    "        foo = gini\n",
    "    else:\n",
    "        foo = entropy\n",
    "    return all_y.shape[0] * foo(all_y) - (left_y.shape[0] * foo(left_y) + right_y.shape[0] * foo(right_y)) / (all_y).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DecisionTreeLeaf:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    y : Тип метки (напр., int или str)\n",
    "        Метка класса, который встречается чаще всего среди элементов листа дерева\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels):\n",
    "        self.prob_distribution = {}\n",
    "        uniq_labels = np.unique(labels)\n",
    "        for x in uniq_labels:\n",
    "            self.prob_distribution[x] = (np.sum(labels == x) / len(labels))\n",
    "        self.y = uniq_labels[np.argmax(list(self.prob_distribution.values()))]\n",
    "\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    \"\"\"\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    split_dim : int\n",
    "        Измерение, по которому разбиваем выборку.\n",
    "    split_value : float\n",
    "        Значение, по которому разбираем выборку.\n",
    "    left : Union[DecisionTreeNode, DecisionTreeLeaf]\n",
    "        Поддерево, отвечающее за случай x[split_dim] < split_value.\n",
    "    right : Union[DecisionTreeNode, DecisionTreeLeaf]\n",
    "        Поддерево, отвечающее за случай x[split_dim] >= split_value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split_dim: int, split_value: float, left, right):\n",
    "        self.split_dim = split_dim\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\"):\n",
    "        # self.X = X\n",
    "        # self.y = y\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.out_of_bag_X = None\n",
    "        self.out_of_bag_y = None\n",
    "\n",
    "\n",
    "    def build_tree(self, X, y, cur_depth):\n",
    "        if len(np.unique(y)) == 1 or X.shape[0] < 2 * self.min_samples_leaf:\n",
    "            return DecisionTreeLeaf(y)\n",
    "        if self.max_depth and cur_depth >= self.max_depth:\n",
    "            return DecisionTreeLeaf(y)\n",
    "\n",
    "        if self.max_features == 'auto':\n",
    "            feature_numb_list = np.random.choice(X.shape[1], int(np.sqrt(X.shape[1])), replace=False)\n",
    "        else:\n",
    "            feature_numb_list = np.random.choice(X.shape[1], self.max_features, replace=False)\n",
    "\n",
    "        new_left, new_right = None, None\n",
    "        X_left, X_right = None, None\n",
    "        split_feature, split_value = None, None\n",
    "        max_gain = 0\n",
    "        for feature in feature_numb_list:\n",
    "            for uni_f in np.unique(X[:, feature]):\n",
    "                left_y = y[X[:, feature] < uni_f]\n",
    "                right_y = y[X[:, feature] >= uni_f]\n",
    "                if len(left_y) < self.min_samples_leaf or len(right_y) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                cur_gain = gain(y, left_y, right_y, criterion=self.criterion)\n",
    "                if cur_gain > max_gain:\n",
    "                    max_gain = cur_gain\n",
    "                    new_left, new_right = left_y, right_y\n",
    "                    X_left, X_right = X[X[:, feature] < uni_f], X[X[:, feature] >= uni_f]\n",
    "                    split_feature = feature\n",
    "                    split_value = uni_f\n",
    "\n",
    "        if max_gain == 0:\n",
    "            return DecisionTreeLeaf(y)\n",
    "        left_node = self.build_tree(X_left, new_left, cur_depth + 1)\n",
    "        right_node = self.build_tree(X_right, new_right, cur_depth + 1)\n",
    "        return DecisionTreeNode(split_feature, split_value, left_node, right_node)\n",
    "\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Строит дерево решений по обучающей выборке.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Обучающая выборка.\n",
    "        y : np.ndarray\n",
    "            Вектор меток классов.\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(X, y, 0)\n",
    "\n",
    "    def go(self, node, x):\n",
    "        if isinstance(node, DecisionTreeLeaf):\n",
    "            return node.prob_distribution\n",
    "        if x[node.split_dim] < node.split_value:\n",
    "            return self.go(node.left, x)\n",
    "        return self.go(node.right, x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = []\n",
    "        for x in X:\n",
    "            proba.append(self.go(self.root, x))\n",
    "        return [max(p.keys(), key=lambda k: p[k]) for p in proba]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверка"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3,\n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "new_tree = DecisionTree()\n",
    "new_tree.fit(X, y)\n",
    "np.mean(new_tree.predict(X) == y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "0.854"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "z8KtlmvBXHIR"
   },
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.trees = []\n",
    "        self.unique = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.unique = np.unique(y)\n",
    "        for i in range(self.n_estimators):\n",
    "            new_tree = DecisionTree(criterion=self.criterion,\n",
    "                                    max_depth=self.max_depth,\n",
    "                                    min_samples_leaf=self.min_samples_leaf,\n",
    "                                    max_features=self.max_features)\n",
    "            ind = np.random.choice(X.shape[0], X.shape[0])\n",
    "            out_ind = list(set(np.arange(X.shape[0])) - set(np.unique(ind)))\n",
    "            # print(out_ind)\n",
    "            new_tree.out_of_bag_X = X[out_ind].copy()\n",
    "            new_tree.out_of_bag_y = y[out_ind].copy()\n",
    "            new_tree.fit(X[ind], y[ind])\n",
    "            self.trees.append(new_tree)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # print(pred)\n",
    "        res = []\n",
    "        for i in range(pred.shape[1]):\n",
    "            count = []\n",
    "            for x in self.unique:\n",
    "                count.append(np.sum(pred[:, i] == x))\n",
    "            res.append(self.unique[np.argmax(count)])\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверка"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.919\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3,\n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=3)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "# print(\"Importance:\", feature_importance(rfc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b41Wii0_XHIR"
   },
   "source": [
    "### Задание 3 (2 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SEa85EniXHIS"
   },
   "outputs": [],
   "source": [
    "def feature_importance(rfc):\n",
    "    res = []\n",
    "    for tree in rfc.trees:\n",
    "        X = tree.out_of_bag_X\n",
    "        y = tree.out_of_bag_y\n",
    "        err_oob = np.mean(tree.predict(X) == y)\n",
    "        errors = []\n",
    "        for j in range(X.shape[1]):\n",
    "            new_X = X.copy()\n",
    "            new_X[:, j] = new_X[:, j][np.random.choice(new_X.shape[0], new_X.shape[0], replace=False)]\n",
    "            err_oob_j = np.mean(tree.predict(new_X) == y)\n",
    "            errors.append(err_oob - err_oob_j)\n",
    "        res.append(errors)\n",
    "    res = np.mean(res, axis=0)\n",
    "    return res\n",
    "\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4VS3WDyXHIT"
   },
   "source": [
    "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате точность должна быть примерно равна `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "wDbhPFWIXHIT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [ 5.13712853e-04 -1.51694042e-04  1.56868444e-01  1.67775870e-01\n",
      "  3.35448314e-01 -1.45176605e-05]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQSKeGmoXHIT"
   },
   "source": [
    "### Задание 4 (1 балл)\n",
    "Теперь поработаем с реальными данными.\n",
    "\n",
    "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\\\n",
    "\\\n",
    "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "xA1GfFJMXHIU"
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    dataframe = pandas.read_csv(path, header=0)\n",
    "    dataset = dataframe.values.tolist()\n",
    "    random.shuffle(dataset)\n",
    "    y_age = [row[0] for row in dataset]\n",
    "    y_sex = [row[1] for row in dataset]\n",
    "    X = [row[2:] for row in dataset]\n",
    "    \n",
    "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "qYKHUy7gXHIU"
   },
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKVys5RMXHIU"
   },
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "LYUV5yZHXHIU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7175283732660782\n",
      "Most important features:\n",
      "1. ovsyanochan\n",
      "2. 4ch\n",
      "3. mudakoff\n",
      "4. rhymes\n",
      "5. styd.pozor\n",
      "6. dayvinchik\n",
      "7. rapnewrap\n",
      "8. pravdashowtop\n",
      "9. tumblr_vacuum\n",
      "10. reflexia_our_feelings\n",
      "11. leprum\n",
      "12. iwantyou\n",
      "13. bot_maxim\n",
      "14. pixel_stickers\n",
      "15. fuck_humor\n",
      "16. i_des\n",
      "17. pozor\n",
      "18. top_screens\n",
      "19. bog_memes\n",
      "20. xfilm\n",
      "CPU times: user 1min 29s, sys: 76.6 ms, total: 1min 29s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(X_train, y_age_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJuzLuVYXHIV"
   },
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "rBGU4dQ_XHIV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8461538461538461\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. modnailru\n",
      "3. girlmeme\n",
      "4. mudakoff\n",
      "5. i_d_t\n",
      "6. cook_good\n",
      "7. 9o_6o_9o\n",
      "8. reflexia_our_feelings\n",
      "9. igm\n",
      "10. thesmolny\n",
      "11. sh.cook\n",
      "12. femalemem\n",
      "13. bon\n",
      "14. be.beauty\n",
      "15. be.women\n",
      "16. rapnewrap\n",
      "17. zerofat\n",
      "18. woman.blog\n",
      "19. recipes40kg\n",
      "20. 4ch\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOO-Lk_0XHIV"
   },
   "source": [
    "### CatBoost\n",
    "В качестве аьтернативы попробуем CatBoost. \n",
    "\n",
    "Устаниовить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\\\n",
    "\\\n",
    "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "X, y = synthetic_dataset(1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "<catboost.core.CatBoostClassifier at 0x7f4f73835ca0>"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = CatBoostClassifier(\n",
    "    custom_loss=['Accuracy'],\n",
    "    random_seed=42,\n",
    "    logging_level='Silent',\n",
    "    loss_function='MultiClass'\n",
    ")\n",
    "rfc.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "mWBdHNJPXHIV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [1.45494257e-02 8.91216472e-03 2.92394951e+01 2.73205004e+01\n",
      " 4.34095828e+01 6.96007876e-03]\n"
     ]
    }
   ],
   "source": [
    "X, y = synthetic_dataset(1000)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test).flatten() == y_test))\n",
    "print(\"Importance:\", rfc.get_feature_importance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj8GnxA7XHIW"
   },
   "source": [
    "### Задание 5 (3 балла)\n",
    "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "HtjJ0jc8XHIW"
   },
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
    "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(X_train, y_age_train, y_sex_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxkWgk-VXHIW"
   },
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "<catboost.core.CatBoostClassifier at 0x7f4f7a1ff7f0>"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = CatBoostClassifier(\n",
    "    custom_loss=['Accuracy'],\n",
    "    random_seed=42,\n",
    "    logging_level='Silent',\n",
    "    loss_function='MultiClass'\n",
    ")\n",
    "rfc.fit(X_train, y_age_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "QNzGCe8kXHIW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7414880201765448\n",
      "Most important features:\n",
      "1. styd.pozor\n",
      "2. ovsyanochan\n",
      "3. 4ch\n",
      "4. mudakoff\n",
      "5. rhymes\n",
      "6. dayvinchik\n",
      "7. leprum\n",
      "8. fuck_humor\n",
      "9. rapnewrap\n",
      "10. xfilm\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test).flatten() == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(rfc.get_feature_importance(), features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIssDLRaXHIW"
   },
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "<catboost.core.CatBoostClassifier at 0x7f4f7a207790>"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = CatBoostClassifier(\n",
    "    custom_loss=['Accuracy'],\n",
    "    random_seed=42,\n",
    "    logging_level='Silent',\n",
    "    loss_function='MultiClass'\n",
    ")\n",
    "rfc.fit(X_train, y_sex_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "nWPSbmKqXHIX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9003783102143758\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. mudakoff\n",
      "3. modnailru\n",
      "4. girlmeme\n",
      "5. 9o_6o_9o\n",
      "6. 4ch\n",
      "7. team\n",
      "8. rapnewrap\n",
      "9. fuck_humor\n",
      "10. academyofman\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test).flatten() == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(rfc.get_feature_importance(), features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "hw09_task.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}